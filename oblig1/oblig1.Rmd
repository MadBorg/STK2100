---
title: "Oblig 1"
output:
  html_notebook: default
  pdf_document: default
---

## Problem 1


```{r, echo = F}
# Getting data
res_bodyfat <- read.csv("res_bodyfat/res_bodyfat.csv")
attach(res_bodyfat)
```

Aim: Find how well bmi, easily computed as the ratio between weight and squared hight (so mesured in kg/m^2), can be used to predict pbfm, whose measurment involves instead a bioelectrical impedance analysis.

### a

Plotting __bnp__ against __pbfm__, and fitting a simple linear model with a summary.

```{r}
plot(bmi, pbfm)
fit.simpleLinear <- lm(pbfm ~ bmi)
print(summary(fit.simpleLinear))
x = (seq(min(bmi), max(bmi), length = 200))
beta = coef(fit.simpleLinear)
lines(x, beta[1] + beta[2]* x, col = 2, lty = 1, lwd = 2)
```
A linear model seems ok in the center of the bmi. But it is quite off in the lower 1/4 and in the upper 1/4.We can see that the intercept and bmi is strongly significant, indicating a strong correlation between bmi and pdfm. The R-squared is siginificant, but not super high, so the model can probably be improved. To see this clearly we plott indicitative plotts on the fitness.

```{r}
plot(fit.simpleLinear)

```

#### Residuals vs Fitted

Shows the Anscombe plot, wich would idealy present a eaven scattering with no patterns in the points. In the plott we see a clear pattern, and is indicating possiple violation of homoscedasicity. Meaning that \epsiplon must be independent of the index.

#### Normal Q-Q

This plot shows quantile-quantile plot. This plot shows the values of the resiuals in increasing order. To see if the residuals folows a normal distribution. For the assumption of normality to be valid we would expect the values to folow the line in the plot. In the plot a heavy tail and head can be observed. So the assumtion of normality is not strong.

#### Scale-Location

#### Residual vs Leverage

## b

We can see that we dont have any negative number, in such cases it is often benefital to use a logarithmic transformation.

```{r}

fit.log <- lm(pbfm ~ log(bmi))
print(summary(fit.log))
beta = coef(fit.log)

x = log(seq(min(bmi), max(bmi), length = 200))
plot(bmi, pbfm)

lines(exp(x),( beta[1] + beta[2]* x), col = 2) 

```

And to see how good the model is we plot some helping plots

```{r}
plot(fit.log)
```


It was also suggested to use a quadratic model.

```{r}
fit.quad<- lm(pbfm ~ bmi + I(bmi^2))
print(summary(fit.quad))
beta = coef(fit.quad)

x = (seq(min(bmi), max(bmi), length = 200))
plot(bmi, pbfm)
#points(bmi, pbfm)
lines(x, beta[1] + beta[2]*x + beta[3]*x^2, col=2)
```
And the acopening helping plots

```{r}
plot(fit.quad)
```

## C - Leave-p-out cross-validation (https://en.wikipedia.org/wiki/Cross-validation_(statistics))

```{r}
max.p = 10
n = 4 # n = len bmi
shuffled = sample(1:length(bmi))
subset.size = length(bmi) %/% n
indices<-matrix(list(), nrow=n, ncol=1)
# indices[[1,1]] = shuffled[1:subset.size]
# indices[[2,1]] = shuffled[subset.size +1:subset.size*2]
# indices[[3,1]] = shuffled[(subset.size*2 + 1):subset.size*3]
# indices[[4,1]] = shuffled[(subset.size*3 + 1):length(bmi)]

for(i in 0:n-1){
  a = (subset.size*i) + 1
  b =  subset.size*(i+1)
  print(c(a,b))
  indices[[i+1,1]] = shuffled[a : b]
}



for (i in 1:max.p){
  for(j in 0:n){
      x = bmi[c(indices[[j %% n]], indices[[(j+1) %% n]], indices[[(j+2) %% n]])]
      y = pbfm[c(indices[[j %% n]], indices[[(j+1) %% n]], indices[[(j+2) %% n]])]
      fit = lm(pbfm ~ poly(x, i), x = TRUE)
      x.test = bmi[]
  }
}
```

## Problem 2

```{r, echo = F}
oral_ca <- read.csv("oral_ca/oral_ca.csv")
attach(oral_ca)
print(summary(oral_ca))
```
Aim of the study was to evaluate the risk of Oral cancer based on the variables __drinks__ (number og 1oz ethanol-equivalent drinks consumed per week), __sex__, __age__ and __cigs__ (number of cigaretts smoked per day).

### We are firs interested in the effect of smoking alone

#### (a)

Q: 

Dichotomize the variable cigs in two categories, smokers and not smokers, and create a table with the observed frequencies for cases and controls. In addition, provide the estimated probabilities (including their standard errors) of experiencing an oral cancer (i.e., being a case) for the two sub-populations.

A:
```{r}
N = nrow(oral_ca)
smokers = subset( oral_ca, oral_ca$cigs > 0 )#smokers = oral_ca[ oral_ca$cigs > 0]
nonSmokers = subset(oral_ca, oral_ca$cigs == 0) #  nonSmokers = oral_ca[ oral_ca$cigs == 0 ]
number.of.cc = nrow( subset(oral_ca,oral_ca$ccstatus == 1) )
number.of.cc.smokers = nrow( subset(oral_ca, oral_ca$ccstatus == 1 & oral_ca$cigs > 0) )
number.of.cc.nonSmokers = nrow(subset(oral_ca, oral_ca$ccstatus == 1 & oral_ca$cigs == 0) )
number.of.somkers = nrow( subset( oral_ca, oral_ca$cigs > 0) )
number.of.nonSmokers = nrow( subset( oral_ca, oral_ca$cigs == 0 ))

# Binomial - Mean
estProb = number.of.cc / N # Number of ppl with cancer devided on ppl (people in the query )
estProbSmoker = number.of.cc.smokers / number.of.somkers # Number of ppl with cancer and smokes, devided on number of ppl who smokes.
estProbNonSmoker = number.of.cc.nonSmokers / number.of.nonSmokers# Number of ppl with cancer and does not smoke, devided on number of ppl who does not soke.

# Binomial Vairance - Var(X) = n*p*(1-p)
estVar = N * estProb * (1 - estProb)
estVarSmoker = number.of.somkers * estProbSmoker * (1 - estProbSmoker)
estVarNonSmoker = number.of.nonSmokers * estProbNonSmoker * (1 - estProbNonSmoker)

cc.smoke.df  = data.frame( 
  Prob = c(estProb, estProbSmoker, estProbNonSmoker), 
  Var = c(estVar, estVarSmoker, estVarNonSmoker),
  row.names = c("General", "Smoker", "NonSmoker")
  )
```

#### (b)

Q:

Test the hypothesis that the two probabilities are equal and comment on the result.

A:

Hypothesis: H0: probSmokers == probNonSmokers against H1: probSmokers != probNonSmokers

```{r}
alfa = 0.05
n1 = number.of.somkers
n2 = number.of.nonSmokers
s1.squared = estVarSmoker
s2.squared = estVarNonSmoker
x1.mean = estProbSmoker
x2.mean = estProbNonSmoker
mu = 0

S.squared = ((n1 -1)*s1.squared + (n2 -1)*s2.squared) / (N -2)
t.obs = ( ( x1.mean - x2.mean ) - abs(mu) ) / sqrt( S.squared * (1/n1 + 1/n2) )
t = c(qt(alfa/2, N), qt(1-(alfa/2), N))
print("t.obs: ")
print(t.obs)
print("t: ")
print(t)


```

We can see that t.obs is well within the borders of t within alfa so we can't say that there is a difference between probSmokers and probNonSmokers, with this test.

#### (c)

Q:

Fit a linear logistic model using the dichotomized variable as explanatory variable and comment on the result: does being a smoker increase or decrease the risk of experiencing oral cancer? How much, in terms of log-odds?

A:

```{r}

y = oral_ca$ccstatus
x = oral_ca$cigs > 0

fit = glm(y ~ x, family = "binomial")



### figure 2.12 (left) ###
# age.c for age continuous
age.c <- bank$age
n_age <- apply(table(y, age.c), 2, sum)
y_age <- as.numeric(table(y, age.c)[2,] / n_age)
plot(sort(unique(age.c)), y_age, ylim = c(0.5, 1), xlim = c(15, 70),
     ylab = "Pr[ Y = high | age ]", xlab = "age", pch = 16, cex = 2)


# fit the model with age as a continuous variable
# with a quadratic effect
mod_tab25up <- glm(y ~ age.c + I(age.c^2), family = 'binomial')
summary(mod_tab25up)

# without a quadratic effect
mod_tab25down <- glm(y ~ age.c, family = 'binomial')
summary(mod_tab25down)

# difference in deviance
diff_deviance <- mod_tab25down$deviance - mod_tab25up$deviance
1 - pchisq(q = diff_deviance, df = 3 - 2) # not significative at level 0.05

### figure 2.12 (b) ###
plot(sort(unique(age.c)), y_age, ylim = c(0.5, 1), xlim = c(15, 70),
     ylab = "Pr[ Y = high | age ]", xlab = "age", pch = 16, cex = sqrt(n_age)/2)
lines(x, predict(mod_tab25down, newdata = data.frame(age.c = x), type = "response"),
      col = 2, lty = 1, lwd = 3)
lines(x, predict(mod_tab25up, newdata = data.frame(age.c = x), type = "response"),
      col = 4, lty = 2, lwd = 3)
legend('bottomright', legend = c("Linear model", "Quadratic model"),
       lty = c(1, 2), col = c(2, 4), cex = 2, lwd = 3)

```













